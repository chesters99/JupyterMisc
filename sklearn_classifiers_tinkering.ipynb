{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# CLASSIFIERS: logistic regression and SVM classifiers (scikit and statsmodel) with boundary plot - GC example\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import preprocessing, grid_search\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.datasets import make_classification, make_blobs, make_gaussian_quantiles\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "def show_details(thetype, model, X, Y, predictions, a, b, intercept):\n",
    "    summary = pd.DataFrame(X)\n",
    "    summary['Yval'] = Y  \n",
    "    summary['predictions'] = (predictions>0.5).astype(int)   # print(\"Pred's\", summary['predictions'].values)\n",
    "    accuracy = model.score(X, Y) if hasattr(model, 'score') else \\\n",
    "        len(summary[summary.predictions == summary.Yval]) / len(summary.predictions)\n",
    "    print('%s Accuracy= %0.4f,' %  (thetype, accuracy), end=' ')\n",
    "    print('Mean= %0.4f, Error Count = %i' % (Y.mean(), len(summary[summary.predictions != summary.Yval] )))\n",
    "    if a and b and intercept and thetype == 'Training':\n",
    "        print('Coefficients: %0.4f %0.4f, Intercept %0.4f' % (a, b, intercept))\n",
    "\n",
    "\n",
    "def plotter(results, Xtrain, Ytrain, Xtest, Ytest, thetitle):\n",
    "    x1plot_min, x1plot_max = min(Xtrain[:,0].min(), Xtest[:,0].min())-0.2, max(Xtrain[:,0].max(), Xtest[:,0].max()) + 0.2\n",
    "    x2plot_min, x2plot_max = min(Xtrain[:,1].min(), Xtest[:,1].min())-0.2, max(Xtrain[:,1].max(), Xtest[:,1].max()) + 0.2\n",
    "    X1inc = (x1plot_max - x1plot_min) / 200; X2inc = (x2plot_max - x2plot_min) / 200;\n",
    "    x1plot, x2plot = np.meshgrid(np.arange(x1plot_min, x1plot_max, X1inc), np.arange(x2plot_min, x2plot_max, X2inc))    \n",
    "    Xplot = np.c_[x1plot.ravel(), x2plot.ravel()]\n",
    "    (model, trainPreds, testPreds) = results\n",
    "    if 'statsmodel' in str(model.__class__):\n",
    "        Xplot = sm.add_constant(Xplot, prepend=True)\n",
    "\n",
    "    plotPreds = model.predict(Xplot).reshape(x1plot.shape)\n",
    "    fig, (ax0, ax1) = plt.subplots(1, 2, sharex=True, sharey=True, figsize=(12,5))\n",
    "    plt.xlim(x1plot_min, x1plot_max); plt.ylim(x2plot_min, x2plot_max); \n",
    "    plt.xlabel('X1'); plt.ylabel('X2'); \n",
    "    ax0.set_title(thetitle + '- Training', fontsize=14); ax0.grid(True)\n",
    "    ax0.contourf(x1plot, x2plot, (plotPreds>0.5).astype(int), cmap=plt.cm.Paired, alpha=0.5)\n",
    "    ax0.scatter(Xtrain[:,0], Xtrain[:,1], c=Ytrain, cmap=plt.cm.Set1)   \n",
    "    ax1.set_title(thetitle + '- Test', fontsize=14); ax1.grid(True)\n",
    "#   ax1.pcolormesh(x1plot, x2plot, plotPreds, cmap=plt.cm.Paired)\n",
    "    ax1.contourf(x1plot, x2plot, (plotPreds>0.5).astype(int), cmap=plt.cm.Paired, alpha=0.5)\n",
    "    ax1.scatter(Xtest[:,0], Xtest[:,1], c=Ytest, cmap=plt.cm.Set1) \n",
    "    try:\n",
    "        a, b, intercept = model.coef_[0][0], model.coef_[0][1], model.intercept_\n",
    "    except (AttributeError, ValueError):\n",
    "        try:\n",
    "            a, b, intercept = model.params[1], model.params[2], model.params[0]  \n",
    "        except AttributeError:\n",
    "            a = b = intercept = None\n",
    "    if a and b and intercept:\n",
    "        Xs = np.linspace(x1plot.min(), x1plot.max(), 40)\n",
    "        ax0.plot(Xs, (-intercept - Xs * a) / b, 'k--')\n",
    "        ax1.plot(Xs, (-intercept - Xs * a) / b, 'k--')\n",
    "    plt.tight_layout(); plt.show() \n",
    "    show_details('Training', model, Xtrain, Ytrain, trainPreds, a, b, intercept)\n",
    "    show_details('Test', model, Xtest, Ytest, testPreds, a, b, intercept)\n",
    "\n",
    "\n",
    "def classify(classifier, Xtrain, Xtest):\n",
    "    if 'statsmodel' in str(classifier.__class__):\n",
    "        model = classifier.fit(disp=False)\n",
    "    else:\n",
    "        model = classifier.fit(Xtrain, Ytrain)\n",
    "    trainPreds = model.predict(Xtrain)\n",
    "    testPreds = model.predict(Xtest)\n",
    "    return model, trainPreds, testPreds\n",
    "    \n",
    "# initialise training and test data sets\n",
    "np.set_printoptions(suppress=True); np.set_printoptions(precision=4); np.set_printoptions(linewidth=105)\n",
    "X, Y = make_classification(n_samples=1000, n_features=2, n_redundant=0, n_informative=1, n_clusters_per_class=1, random_state=14)\n",
    "X[:,0] *= 30 # to alter scaling of data (so scaling input b4 in regression shows benefit)\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.5, random_state=0)\n",
    "Xtrainint = sm.add_constant(Xtrain, prepend = True)\n",
    "Xtestint = sm.add_constant(Xtest, prepend = True)\n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "Xtrainscaled = scaler.transform(Xtrain)\n",
    "Xtestscaled = scaler.transform(Xtest)\n",
    "\n",
    "resultsLogR = classify(LogisticRegression(C = 0.1), Xtrain, Xtest)\n",
    "plotter(resultsLogR, Xtrain, Ytrain, Xtest, Ytest, 'Logistic Regression Scikit')\n",
    "resultsLogR2 = classify(sm.Logit(Ytrain, Xtrainint), Xtrainint, Xtestint)\n",
    "plotter(resultsLogR2, Xtrain, Ytrain, Xtest, Ytest, 'Logistic Regression Statsmodel')\n",
    "resultsSVC = classify(SVC(C=1), Xtrain, Xtest)\n",
    "plotter(resultsSVC, Xtrain, Ytrain, Xtest, Ytest, 'SVM Classifier - not scaled')\n",
    "resultsSVCsc = classify(SVC(C=10), Xtrainscaled, Xtestscaled)\n",
    "plotter(resultsSVCsc, Xtrainscaled, Ytrain, Xtestscaled, Ytest, 'SVM Classifier - Scaled')\n",
    "resultsSVR = classify(SVR(C=1), Xtrain, Xtest)\n",
    "plotter(resultsSVR, Xtrain, Ytrain, Xtest, Ytest, 'SVM Regression - not scaled')\n",
    "resultsSVRsc = classify(SVR(C=10), Xtrainscaled, Xtestscaled)\n",
    "plotter(resultsSVRsc, Xtrainscaled, Ytrain, Xtestscaled, Ytest, 'SVM Regression - Scaled')\n",
    "\n",
    "resultsDtree = classify(DecisionTreeClassifier(), Xtrain, Xtest)\n",
    "plotter(resultsDtree, Xtrain, Ytrain, Xtest, Ytest, 'Decision Tree Classifier')\n",
    "resultsGNB = classify(GaussianNB(), Xtrain, Xtest)\n",
    "plotter(resultsGNB, Xtrain, Ytrain, Xtest, Ytest, 'Gaussian Naive Bayes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# try many classifiers with many C values and scaled and non-scaled data (Grid Search example)\n",
    "\n",
    "\n",
    "def gridSearch(parameters):\n",
    "    for model in [resultsLogR[0], resultsSVC[0], resultsSVCsc[0], resultsSVR[0], resultsSVRsc[0]]:\n",
    "        grid = grid_search.GridSearchCV(model, parameters)\n",
    "    \n",
    "        grid.fit(Xtrain, Ytrain)\n",
    "        print(\"Best parameters set on Non-Scaled training set for %s are %s:\" % (model.__class__.__name__, grid.best_params_))\n",
    "        print(\"Grid scores on training set:\")\n",
    "        for params, mean_score, scores in grid.grid_scores_:\n",
    "            print(\"%6.3f (+/-%0.3f) for %r\" % (mean_score, scores.std() * 2, params))\n",
    "        print(\"Detailed classification report on training set:\")\n",
    "        y_true, y_pred = Ytest, (grid.predict(Xtest)>0.5).astype(int)\n",
    "        print(classification_report(y_true, y_pred))\n",
    "    \n",
    "        grid.fit(Xtrainscaled, Ytrain)\n",
    "        print(\"Best arameters set on Scaled training set for %s are %s:\" % (model.__class__.__name__, grid.best_params_))\n",
    "        print(\"Grid scores on training set:\")\n",
    "        for params, mean_score, scores in grid.grid_scores_:\n",
    "            print(\"%6.3f (+/-%0.3f) for %r\" % (mean_score, scores.std() * 2, params))\n",
    "        print(\"Detailed classification report on training set:\")\n",
    "        y_true, y_pred = Ytest, (grid.predict(Xtestscaled)>0.5).astype(int)\n",
    "        print(classification_report(y_true, y_pred))\n",
    "    \n",
    "        \n",
    "def result(model, thetype, trainScore, testScore, trainPreds, testPreds):\n",
    "    global dfr\n",
    "    trainErrors = sum(Ytrain!=trainPreds)\n",
    "    testErrors = sum(Ytest!=testPreds)\n",
    "    rating = testErrors + trainErrors + abs(testErrors - trainErrors)\n",
    "    C = model.C if hasattr(model,'C') else np.NaN\n",
    "    dfr = dfr.append({'model': model.__class__.__name__[:12], 'C': C, 'scale': thetype, 'trainScore': trainScore, \n",
    "                'testScore': testScore, 'trainErrs': trainErrors, 'testErrs': testErrors, 'rating': rating}, ignore_index=True)\n",
    " \n",
    "    \n",
    "def manualSearch(parameters):\n",
    "    classifiers = []\n",
    "    for clf in [LogisticRegression, SVC, SVR,]:\n",
    "        for i, C in enumerate(parameters['C']):\n",
    "            classifiers.append(clf(C=C))\n",
    "    classifiers.append(DecisionTreeClassifier())\n",
    "    classifiers.append(GaussianNB())\n",
    "        \n",
    "    for clf in classifiers:\n",
    "        model = clf.fit(Xtrain, Ytrain)\n",
    "        trainScore = model.score(Xtrain, Ytrain)\n",
    "        testScore  = model.score(Xtest, Ytest)\n",
    "        trainPreds = (model.predict(Xtrain)>0.5).astype(int)\n",
    "        testPreds  = (model.predict(Xtest)>0.5).astype(int)\n",
    "        result(model, 'not scaled', trainScore, testScore, trainPreds, testPreds)\n",
    "\n",
    "        modelScaled = clf.fit(Xtrainscaled, Ytrain)\n",
    "        trainScore = modelScaled.score(Xtrainscaled, Ytrain)\n",
    "        testScore = modelScaled.score(Xtestscaled, Ytest)\n",
    "        trainPreds = (modelScaled.predict(Xtrainscaled)>0.5).astype(int)\n",
    "        testPreds = (modelScaled.predict(Xtestscaled)>0.5).astype(int)\n",
    "        result(model, 'Scaled', trainScore, testScore, trainPreds, testPreds)\n",
    "\n",
    "\n",
    "dfr = pd.DataFrame()\n",
    "params = {'C':[0.01, 0.1, 1, 10, 100, 1000], }\n",
    "#gridSearch(params)\n",
    "manualSearch(params)\n",
    "dfr = dfr[['model', 'C', 'scale', 'trainScore', 'testScore', 'trainErrs', 'testErrs', 'rating']]\n",
    "print('Best Model and Params:  \\n', dfr[dfr.rating == min(dfr.rating)])\n",
    "dfr.sort_values(['rating','testErrs','C'], ascending=[1,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Generate Data examples - from internet\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification, make_blobs, make_gaussian_quantiles\n",
    "\n",
    "plt.figure(figsize=(13, 13))\n",
    "plt.subplots_adjust(bottom=.05, top=.9, left=.05, right=.95)\n",
    "seed = np.random.RandomState()\n",
    "\n",
    "plt.subplot(321)\n",
    "plt.title(\"One informative feature, one cluster per class\", fontsize='small')\n",
    "X1, Y1 = make_classification(n_features=2, n_redundant=0, n_informative=2, n_clusters_per_class=1, n_classes=3, random_state=seed)\n",
    "plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1, cmap=plt.cm.Paired) \n",
    "\n",
    "plt.subplot(322)\n",
    "plt.title(\"Two informative features, one cluster per class\", fontsize='small')\n",
    "X1, Y1 = make_classification(n_features=2, n_redundant=0, n_informative=2, n_clusters_per_class=1, random_state=seed)\n",
    "plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1, cmap=plt.cm.Paired)\n",
    "\n",
    "plt.subplot(323)\n",
    "plt.title(\"Two informative features, two clusters per class\", fontsize='small')\n",
    "X2, Y2 = make_classification(n_features=2, n_redundant=0, n_informative=2, random_state=seed)\n",
    "plt.scatter(X2[:, 0], X2[:, 1], marker='o', c=Y1, cmap=plt.cm.Paired)\n",
    "\n",
    "plt.subplot(324)\n",
    "plt.title(\"Multi-class, two informative features, one cluster\", fontsize='small')\n",
    "X1, Y1 = make_classification(n_features=2, n_redundant=0, n_informative=2, n_clusters_per_class=1, n_classes=3, random_state=seed)\n",
    "plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1, cmap=plt.cm.Paired)\n",
    "\n",
    "plt.subplot(325)\n",
    "plt.title(\"Three blobs\", fontsize='small')\n",
    "X1, Y1 = make_blobs(n_features=2, centers=3, random_state=seed)\n",
    "plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1, cmap=plt.cm.Paired)\n",
    "\n",
    "plt.subplot(326)\n",
    "plt.title(\"Gaussian divided into three quantiles\", fontsize='small')\n",
    "X1, Y1 = make_gaussian_quantiles(n_features=2, n_classes=3, random_state=seed)\n",
    "plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1, cmap=plt.cm.Paired)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#plt.figure(figsize=(13, 33))\n",
    "#plt.subplots_adjust(bottom=.05, top=.9, left=.05, right=.95)\n",
    "#for i in range(1,30):\n",
    "#    plt.subplot(10,3,i)\n",
    "#    plt.title(\"One informative feature, one cluster per class \"+str(i), fontsize='small')\n",
    "#    X1, Y1 = make_classification(n_features=2, n_redundant=0, n_informative=1, n_clusters_per_class=1, random_state=i)\n",
    "#    plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1, cmap=plt.cm.Paired)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Linear regression - GC example\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "np.set_printoptions(suppress=True); np.set_printoptions(precision=4); np.set_printoptions(linewidth=105)\n",
    "\n",
    "np.random.seed(5)\n",
    "samples=9\n",
    "Xlin = pd.DataFrame(np.random.rand(samples,1)) * 10\n",
    "Ylin = Xlin * (np.random.rand(samples,1)*0.5 + 1) + np.random.rand()*3\n",
    "\n",
    "#scikit linear regression\n",
    "LinR = LinearRegression(normalize=True)\n",
    "modelLin = LinR.fit(Xlin, Ylin)\n",
    "a = float(modelLin.coef_[0])\n",
    "intercept= float(modelLin.intercept_)\n",
    "fn = np.poly1d([a, intercept+0.1])  # add tiny bit to intercept so appears on plot\n",
    "print('scikit coefficient %0.4f Intercept %0.4f, Accuracy=%.4f' % (a, intercept, modelLin.score(Xlin, Ylin)))\n",
    "\n",
    "#scipy linear regression\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(np.array(Xlin)[:,0], np.array(Ylin)[:,0])\n",
    "print('Scipy linregress, Slope=%.4f, Incpt=%.4f, R-sqd=%.4f, p-value=%.4f, Std Err=%.4f\\n' %\n",
    "     (slope, intercept, r_value ** 2, p_value, std_err))\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "Xs = np.linspace(-1, 10, 100)\n",
    "plt.xlabel('x'); plt.ylabel('y')\n",
    "plt.xlim(0, 10); plt.ylim(3, 17); plt.grid(True)\n",
    "plt.plot(Xs, Xs * a + intercept -0.1, 'r--', label='Linear') # add tiny bit to ensure appears on plot\n",
    "plt.plot(Xs, fn(Xs), 'g--', label = 'Linear f(n)')\n",
    "plt.scatter(Xlin, Ylin)\n",
    "\n",
    "# try different degrees of polynomial to fit\n",
    "for degree in range(1,8): \n",
    "    theta = np.polyfit(x=Xlin[0], y=Ylin[0], deg=degree)\n",
    "    fn = np.poly1d(theta) # plt.plot(Xs, fn(Xs))\n",
    "    plt.plot(Xs, np.polyval(theta, Xs), label='degree %s' % degree)\n",
    "    err = np.sqrt(np.sum((np.polyval(theta, Xlin) -Ylin)**2)[0]/Ylin.count())\n",
    "    r2 = r2_score(Ylin, fn(Xlin) )\n",
    "    print('Polyfit degree %i, Err %0.3f, R-sqd %0.4f, Theta %s' % (degree, err, r2, theta))\n",
    "    \n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "\n",
    "# Linear Regression using statsmodel gives same results\n",
    "Xlin1 = sm.add_constant(Xlin, prepend=False)\n",
    "Ylin1 = Ylin\n",
    "OLS = sm.OLS(Ylin1, Xlin1)\n",
    "modelOLS = OLS.fit()\n",
    "print (modelOLS.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# manual matrix driven linear regression manually using gradient descent (from Coursera machine learning - Stanford Uni)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def J_cost(X, y, theta):\n",
    "    m, _ = X.shape\n",
    "    predictions = X * theta\n",
    "    square_errors = np.square(predictions - y)\n",
    "    J = 1/(2*m) * sum(square_errors)\n",
    "    return round(float(J), 4)\n",
    "\n",
    "filename = 'ex1data1.txt'\n",
    "df = pd.read_csv(filename, header=None, names=['d1','d2'] )\n",
    "X = np.matrix(df.d1).T\n",
    "m, _ = X.shape\n",
    "X = np.matrix(np.concatenate((np.ones((m,1)), X), axis=1))\n",
    "y = np.matrix(df.d2).T\n",
    "theta = np.matrix(np.zeros((2,1)))\n",
    "alpha = 0.01\n",
    "iterations = 1500\n",
    "J = []\n",
    "\n",
    "for i in range(iterations):\n",
    "    h =  X * theta\n",
    "    e = h - y\n",
    "    adj = X.T * e * alpha * 1 / m\n",
    "    theta = theta - adj\n",
    "    J.append( J_cost(X, y, theta) )\n",
    "\n",
    "print('Gradient Descent Theta ', str(theta.T))\n",
    "# calculate linear regression theta using normal equation\n",
    "theta1 = (X.T * X).I * X.T * y\n",
    "print('Normal Equation Theta ',theta1.T)\n",
    "\n",
    "Xnew = df.d1.values\n",
    "theta2 = np.polyfit(x=Xnew, y=y, deg=1)\n",
    "print('Scipy Theta ',theta2[1], theta2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fitting multiple polynomials using scikit learn pipeline - example from internet\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "%matplotlib inline\n",
    "\n",
    "def f(x):\n",
    "    return np.sin(2 * np.pi * x)\n",
    "\n",
    "# generate points used to plot and display them\n",
    "x_plot = np.linspace(0, 1, 100)\n",
    "np.random.seed(9)\n",
    "n_samples = 100\n",
    "X = np.random.uniform(0, 1, size=n_samples)[:, np.newaxis]\n",
    "y = f(X) + np.random.normal(scale=0.3, size=n_samples)[:, np.newaxis]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.plot(x_plot, f(x_plot), color='green')\n",
    "ax.scatter(X_train, y_train, s=10)\n",
    "ax.set_ylim((-2, 2)); ax.set_xlim((0, 1))\n",
    "ax.set_ylabel('y'); ax.set_xlabel('x')\n",
    "\n",
    "# fit different polynomials and plot approximations\n",
    "fig, axes = plt.subplots(5, 2, figsize=(15, 15))\n",
    "for ax, degree in zip(axes.ravel(), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]):\n",
    "    est = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "    est.fit(X_train, y_train)\n",
    "    ax.plot(x_plot, f(x_plot), color='green')\n",
    "    ax.scatter(X_train, y_train, s=10)\n",
    "    ax.plot(x_plot, est.predict(x_plot[:, np.newaxis]), color='red', label='degree=%d' % degree)\n",
    "    ax.set_ylim((-2, 2)); ax.set_xlim((0, 1))\n",
    "    ax.set_ylabel('y'); ax.set_xlabel('x')\n",
    "    ax.legend(loc='upper right',fontsize='small')  #, fontsize='small')\n",
    "    \n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# plot train and test error for each degree\n",
    "train_error = np.empty(10); test_error = np.empty(10)\n",
    "for degree in range(10):\n",
    "    est = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "    est.fit(X_train, y_train)\n",
    "    train_error[degree] = mean_squared_error(y_train, est.predict(X_train))\n",
    "    test_error[degree] = mean_squared_error(y_test, est.predict(X_test))\n",
    "    print('degree %s train error %0.3f test error %0.4f' % (degree, train_error[degree] , test_error[degree]))\n",
    "\n",
    "plt.plot(np.arange(10), train_error, color='green', label='train')\n",
    "plt.plot(np.arange(10), test_error, color='red', label='test')\n",
    "plt.ylim((0.0, 1e0))\n",
    "plt.ylabel('log(mean squared error)'); plt.xlabel('degree')\n",
    "plt.legend(loc='lower left')\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# comparision of Scikit learn classifiers (from internet)\n",
    "\n",
    "from sklearn import cross_validation\n",
    "# Logistic Regression\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# load the iris datasets\n",
    "dataset = datasets.load_iris()\n",
    "# fit a logistic regression model to the data\n",
    "model = LogisticRegression()\n",
    "model.fit(dataset.data, dataset.target)\n",
    "print(model)\n",
    "# make predictions\n",
    "expected = dataset.target\n",
    "predicted = model.predict(dataset.data)\n",
    "# summarize the fit of the model\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))\n",
    "\n",
    "# Gaussian Naive Bayes\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# load the iris datasets\n",
    "dataset = datasets.load_iris()\n",
    "# fit a Naive Bayes model to the data\n",
    "model = GaussianNB()\n",
    "model.fit(dataset.data, dataset.target)\n",
    "print(model)\n",
    "# make predictions\n",
    "expected = dataset.target\n",
    "predicted = model.predict(dataset.data)\n",
    "# summarize the fit of the model\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))\n",
    "\n",
    "# k-Nearest Neighbor\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# load iris the datasets\n",
    "dataset = datasets.load_iris()\n",
    "# fit a k-nearest neighbor model to the data\n",
    "model = KNeighborsClassifier()\n",
    "model.fit(dataset.data, dataset.target)\n",
    "print(model)\n",
    "# make predictions\n",
    "expected = dataset.target\n",
    "predicted = model.predict(dataset.data)\n",
    "# summarize the fit of the model\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))\n",
    "\n",
    "# Decision Tree Classifier\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# load the iris datasets\n",
    "dataset = datasets.load_iris()\n",
    "# fit a CART model to the data\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(dataset.data, dataset.target)\n",
    "print(model)\n",
    "# make predictions\n",
    "expected = dataset.target\n",
    "predicted = model.predict(dataset.data)\n",
    "# summarize the fit of the model\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))\n",
    "\n",
    "# Support Vector Machine\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "# load the iris datasets\n",
    "dataset = datasets.load_iris()\n",
    "# fit a SVM model to the data\n",
    "model = SVC()\n",
    "model.fit(dataset.data, dataset.target)\n",
    "print(model)\n",
    "# make predictions\n",
    "expected = dataset.target\n",
    "predicted = model.predict(dataset.data)\n",
    "# summarize the fit of the model\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))\n",
    "\n",
    "#K-fold cross-validation ############################################################################\n",
    "\n",
    "# We give cross_val_score a model, the entire data set and its \"real\" values, and the number of folds:\n",
    "scores = cross_validation.cross_val_score(model, dataset.data, dataset.target, cv=5)\n",
    "\n",
    "# Print the accuracy for each fold and the mean\n",
    "print('\\nK-fold cross validation scores on SVC', scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# neural network - only in 0.18dev version of sklearn so cant be run :(\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "X = [[0., 0.], [1., 1.]]\n",
    "y = [0, 1]\n",
    "clf = MLPClassifier(algorithm='l-bfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "clf.fit(X, y) \n",
    "\n",
    "clf.predict([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Single Decision Tree and Random Forest classifiers, with Graphviz output of trees\n",
    "\n",
    "from sklearn import datasets, tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from IPython.display import Image, display\n",
    "from sklearn.externals.six import StringIO  \n",
    "import pydotplus as pydot\n",
    "\n",
    "dataset = datasets.load_iris()\n",
    "\n",
    "# first try single decision treee\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(dataset.data, dataset.target)\n",
    "print ('prediction=', clf.predict([[3, 1, 4, 2]]))\n",
    "dot_data = StringIO()\n",
    "tree.export_graphviz(clf, out_file=dot_data, feature_names=dataset.feature_names)  \n",
    "graph = pydot.graph_from_dot_data(dot_data.getvalue())  \n",
    "display(Image(graph.create_png()))\n",
    "\n",
    "\n",
    "# then try random forest\n",
    "clf1 = RandomForestClassifier(n_estimators=2000)\n",
    "clf1 = clf1.fit(dataset.data, dataset.target)\n",
    "for i in range(10):\n",
    "    dot_data = StringIO()  \n",
    "    tree.export_graphviz(clf1.estimators_[i], out_file=dot_data, feature_names=dataset.feature_names)  \n",
    "    graph = pydot.graph_from_dot_data(dot_data.getvalue())  \n",
    "    display(Image(graph.create_png(), width=400, height=200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nbpresent": {
   "slides": {
    "4a8c2ffe-5649-4a97-9219-08d5c8cc0bf1": {
     "id": "4a8c2ffe-5649-4a97-9219-08d5c8cc0bf1",
     "prev": "6fca60b6-77a4-437b-9559-9f329df66e3b",
     "regions": {
      "063a9150-5764-4105-adcb-6326effc3ab5": {
       "attrs": {
        "height": 0.4,
        "width": 0.8,
        "x": 0.1,
        "y": 0.5
       },
       "content": {
        "cell": "15901a67-35a6-4625-a8b1-46258846df31",
        "part": "whole"
       },
       "id": "063a9150-5764-4105-adcb-6326effc3ab5"
      },
      "2a12c131-7934-43ed-9b19-f395cfb6b467": {
       "attrs": {
        "height": 0.4,
        "width": 0.8,
        "x": 0.1,
        "y": 0.5
       },
       "content": {
        "cell": "a0bebf51-25d6-4854-8df2-c9b4da1b6e0c",
        "part": "whole"
       },
       "id": "2a12c131-7934-43ed-9b19-f395cfb6b467"
      },
      "46e40a19-cadf-45a4-8d8f-5fdd092729cd": {
       "attrs": {
        "height": 0.4,
        "width": 0.8,
        "x": 0.1,
        "y": 0.5
       },
       "content": {
        "cell": "69bf469f-8e69-489d-a731-ce9526170aba",
        "part": "whole"
       },
       "id": "46e40a19-cadf-45a4-8d8f-5fdd092729cd"
      },
      "576682ac-1b6b-4b9c-9b6d-9bdbc8dd33a8": {
       "attrs": {
        "height": 0.4,
        "width": 0.8,
        "x": 0.1,
        "y": 0.5
       },
       "content": {
        "cell": "95919a95-8d25-4cdb-826f-4c52842d90d1",
        "part": "whole"
       },
       "id": "576682ac-1b6b-4b9c-9b6d-9bdbc8dd33a8"
      },
      "8ce7de4b-b258-4c33-8c82-b837ba4fe97b": {
       "attrs": {
        "height": 0.4,
        "width": 0.8,
        "x": 0.1,
        "y": 0.5
       },
       "content": {
        "cell": "adbbb3fe-5bf4-4bcd-adee-38415e003e70",
        "part": "whole"
       },
       "id": "8ce7de4b-b258-4c33-8c82-b837ba4fe97b"
      },
      "94faa8b4-578c-4dce-8d08-69f93b821fe7": {
       "attrs": {
        "height": 0.4,
        "width": 0.8,
        "x": 0.1,
        "y": 0.5
       },
       "content": {
        "cell": "fd656db1-9daf-4bfd-ba50-ff4710752bd0",
        "part": "whole"
       },
       "id": "94faa8b4-578c-4dce-8d08-69f93b821fe7"
      },
      "9a4dc549-238c-4470-adf0-f6b0e3ae3625": {
       "attrs": {
        "height": 0.4,
        "width": 0.8,
        "x": 0.1,
        "y": 0.5
       },
       "content": {
        "cell": "5d2019b8-08e6-4a08-983b-e02dcb0f835b",
        "part": "whole"
       },
       "id": "9a4dc549-238c-4470-adf0-f6b0e3ae3625"
      },
      "af6ea0f2-7b31-4a6a-a3e6-721ddb8f7cb2": {
       "attrs": {
        "height": 0.4,
        "width": 0.8,
        "x": 0.1,
        "y": 0.5
       },
       "content": {
        "cell": "e9147c58-b481-4c95-9dee-644ae81fc940",
        "part": "whole"
       },
       "id": "af6ea0f2-7b31-4a6a-a3e6-721ddb8f7cb2"
      },
      "b09fb11d-eb0d-4ab7-833f-f8f99c25aa6b": {
       "attrs": {
        "height": 0.4,
        "width": 0.8,
        "x": 0.1,
        "y": 0.5
       },
       "content": {
        "cell": "d5219b38-d0bc-4cdf-aef4-6c23962cc9e7",
        "part": "whole"
       },
       "id": "b09fb11d-eb0d-4ab7-833f-f8f99c25aa6b"
      },
      "b2c81474-9515-4198-9e08-67bc7724fc6c": {
       "attrs": {
        "height": 0.4,
        "width": 0.8,
        "x": 0.1,
        "y": 0.5
       },
       "content": {
        "cell": "f987982f-74b8-4bca-a1d7-6da9b49ce0ac",
        "part": "whole"
       },
       "id": "b2c81474-9515-4198-9e08-67bc7724fc6c"
      },
      "bc032d91-7b60-4e27-9d5c-8dd8bf603415": {
       "attrs": {
        "height": 0.4,
        "width": 0.8,
        "x": 0.1,
        "y": 0.5
       },
       "content": {
        "cell": "f2890ae2-0240-4a4a-9b4b-88f02e8b1a9a",
        "part": "whole"
       },
       "id": "bc032d91-7b60-4e27-9d5c-8dd8bf603415"
      },
      "e2d4bf79-3336-4bb5-8a60-77152bc2eb35": {
       "attrs": {
        "height": 0.4,
        "width": 0.8,
        "x": 0.1,
        "y": 0.5
       },
       "content": {
        "cell": "782f00b2-9afc-49e7-a2e2-2af5fa60bed1",
        "part": "whole"
       },
       "id": "e2d4bf79-3336-4bb5-8a60-77152bc2eb35"
      },
      "f184b463-e908-404d-8313-4a93ac63eeb6": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "73d0c19d-e28e-451f-97ee-fe293f59782c",
        "part": "whole"
       },
       "id": "f184b463-e908-404d-8313-4a93ac63eeb6"
      }
     }
    },
    "6fca60b6-77a4-437b-9559-9f329df66e3b": {
     "id": "6fca60b6-77a4-437b-9559-9f329df66e3b",
     "prev": "b7ab1f3f-af37-4a75-88ac-c00dde9b7e48",
     "regions": {
      "ac1b1ca7-6008-40b4-b5ab-bdbaa243d8e8": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "e7b3af93-b294-41a6-a9e4-eac1859099d7",
        "part": "whole"
       },
       "id": "ac1b1ca7-6008-40b4-b5ab-bdbaa243d8e8"
      }
     }
    },
    "b7ab1f3f-af37-4a75-88ac-c00dde9b7e48": {
     "id": "b7ab1f3f-af37-4a75-88ac-c00dde9b7e48",
     "prev": null,
     "regions": {
      "9fbe63fb-d101-4fa6-b2a7-7dee19057c5d": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "198e60d7-73b8-48a6-afbe-33fd9919ad62",
        "part": "whole"
       },
       "id": "9fbe63fb-d101-4fa6-b2a7-7dee19057c5d"
      }
     }
    }
   },
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
